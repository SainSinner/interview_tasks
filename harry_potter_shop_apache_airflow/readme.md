## Решение кейса. Реализация регулярного ETL-потока **AirFlow**. Lookup csv файла с обращением к API стороннего сервиса.  
### Сама задача звучит следующим образом:  
"  
Вы работаете в компании, которая производит и продаёт фигурки персонажей Гарри Поттера.
В файле [sales.csv](sales.csv) сгенерирована история продаж. Этот файл можно пополнять при помощи python скриптов ([genOneDay.py](genOneDay.py), [genFewDays.py](genFewDays.py), [genManyDays.py](genManyDays.py)), имитируя поступление свежих данных (генерация идёт целыми днями, по 1 или по нескольку дней за 1 запуск).
Кроме того, имеется [сторонняя база данных](https://hp-api.onrender.com/), которая предоставляет REST API интерфейс, и имитирует расширенный набор мастер данных, детально описывающий каждую SKU.

Неободимо реализовать ETL поток в среде Nifi, который позволит считывать факт продаж из [sales.csv](sales.csv), группировать его до детализации sale_date - house - amount - last_verstamp, и запишет результаты в файл [house_sales](house_sales). 
House - атрибут персонажа, характеризующий Факультет, к которому тот относится, и информация о котором есть только в API.

Реализация должна учитывать, что при поступлении свежих данных в [sales.csv](sales.csv), поток при очередном запуске должен забирать только новую информацию, и дозаписывать её в [house_sales](house_sales) (допускается вариант с генерацией нового файла при каждом запуске, рядом с уже существующими, но заполнение нового файла должно происходить только теми данными, которых не было во всех предыдущих запусках). 
Кроме того, мастер данные по SKU могут меняться (как бы это не было нелепо в контексте тематики, но представим себе, что операторы, заполняющие мастер данные, могут ошибиться, и в какой-то момент поменять атрибут персонажа), а значит обращаться в REST API нужно при каждом запуске потока. 
"  

### [Мое решение](main_dag.py)

Повествование будет представлено по шагам последовательности котороая должна соблюдаться для корректной работы потока. 

![UI Airflow harry_potter_shop_apache_airflow](PNG%2Fharry_potter_shop_apache_airflow_ui.png)

[Файл](main_dag.py) самого DAG.
[docker-compose.yml](docker-compose.yml) и [Dockerfile](Dockerfile) в которые были собраны и в которых были подняты образы в папке где они располагались командой ниже:
```bash
sudo docker-compose build --no-cache && sudo docker-compose up -d
```
1. **create_file_key_verstamp**. Эта функция создает файл в котором мы будем хранить наиболее актуальный key_verstamp последней выгрузки.
2. **compute_house_sales**. Эта функция читатет key_verstamp последней выгрузки.
3. **compute_house_sales**. Читает sales.csv и фильтрует его по key_verstamp последней выгрузки, отбирая строки свежее чем key_verstamp последней выгрузки.
4. **compute_house_sales**. Обращается к внешнему API функции /characters и оставляет только нужные нам столбцы ["id", "name", "house"].
5. **compute_house_sales**. Джоинит информацию от API функции /characters с информацией sales.csv, применяет к нему агрегацию для ответа на поставленный вопрос выше.
6. **compute_house_sales**. Создает CSV с результатом запроса в house_sales.
7. **update_actual_key_verstamp**. Читает данные из sales.csv и выбирает наиболее свежий key_verstamp, записывая его в key_verstamp.csv.

**update_actual_key_verstamp** - можно не выносить в отдельную функцию от **compute_house_sales**, так даже правильнее, потому что пока мы выполняем **compute_house_sales** в нее могут быть добавлены новые строки, но десь "растянуто" для демонстрации.

